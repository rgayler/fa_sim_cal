<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Ross Gayler" />


<title>Notes</title>

<script src="site_libs/header-attrs-2.6/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="site_libs/viz-1.8.2/viz.js"></script>
<link href="site_libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="site_libs/grViz-binding-1.0.6.1/grViz.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<link rel="icon" href="https://github.com/workflowr/workflowr-assets/raw/master/img/reproducible.png">
<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>



<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Frequency-Aware Similarity Calibration</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/rgayler/fa_sim_cal">
    <span class="fas fa-github"></span>
     
    Source code
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Notes</h1>
<h4 class="author">Ross Gayler</h4>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span>
workflowr
<span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks">
Checks <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2021-02-19
</p>
<p>
<strong>Checks:</strong>
<span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
7
<span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span>
0
</p>
<p>
<strong>Knit directory:</strong>
<code>fa_sim_cal/</code>
<span class="glyphicon glyphicon-question-sign" aria-hidden="true" title="This is the local directory in which the code in this file was executed.">
</span>
</p>
<p>
This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a>
analysis was created with <a
  href="https://github.com/jdblischak/workflowr">workflowr</a> (version
1.6.2). The <em>Checks</em> tab describes the
reproducibility checks that were applied when the results were created.
The <em>Past versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate">
<span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
<strong>R Markdown file:</strong> up-to-date
</a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate" class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git repository, you
know the exact version of the code that produced these results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty">
<span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
<strong>Environment:</strong> empty
</a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the global
environment can affect the analysis in your R Markdown file in unknown ways.
For reproduciblity it’s best to always run the code in an empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20201104code">
<span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
<strong>Seed:</strong> <code>set.seed(20201104)</code>
</a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20201104code" class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20201104)</code> was run prior to running the code in the R Markdown file.
Setting a seed ensures that any results that rely on randomness, e.g.
subsampling or permutations, are reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded">
<span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
<strong>Session information:</strong> recorded
</a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package versions is
critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone">
<span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
<strong>Cache:</strong> none
</a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be confident
that you successfully produced the results during this run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative">
<span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
<strong>File paths:</strong> relative
</a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr project
makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomrgaylerfasimcaltree815af1ebb85d85305cf6efd60ef55ed943be66c3targetblank815af1ea">
<span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
<strong>Repository version:</strong> <a href="https://github.com/rgayler/fa_sim_cal/tree/815af1ebb85d85305cf6efd60ef55ed943be66c3" target="_blank">815af1e</a>
</a>
</p>
</div>
<div id="strongRepositoryversionstrongahrefhttpsgithubcomrgaylerfasimcaltree815af1ebb85d85305cf6efd60ef55ed943be66c3targetblank815af1ea" class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development and
connecting the code version to the results is critical for reproducibility.
</p>
<p>
The results in this page were generated with repository version <a href="https://github.com/rgayler/fa_sim_cal/tree/815af1ebb85d85305cf6efd60ef55ed943be66c3" target="_blank">815af1e</a>.
See the <em>Past versions</em> tab to see a history of the changes made to the
R Markdown and HTML files.
</p>
<p>
Note that you need to be careful to ensure that all relevant files for the
analysis have been committed to Git prior to generating the results (you can
use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only
checks the R Markdown file, but you know if there are other scripts or data
files that it depends on. Below is the status of the Git repository when the
results were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .Rhistory
    Ignored:    .Rproj.user/
    Ignored:    .tresorit/
    Ignored:    data/VR_20051125.txt.xz
    Ignored:    output/blk_char.fst
    Ignored:    output/ent_blk.fst
    Ignored:    output/ent_cln.fst
    Ignored:    output/ent_raw.fst
    Ignored:    renv/library/
    Ignored:    renv/staging/

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in
this status report because it is ok for generated content to have uncommitted
changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the repository in which changes were made
to the R Markdown (<code>analysis/notes.Rmd</code>) and HTML (<code>docs/notes.html</code>)
files. If you’ve configured a remote Git repository (see
<code>?wflow_git_remote</code>), click on the hyperlinks in the table below to
view the files as they were in that past version.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/815af1ebb85d85305cf6efd60ef55ed943be66c3/analysis/notes.Rmd" target="_blank">815af1e</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-02-19
</td>
<td>
Add sampling to notes
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/a2cde2d5b9b50d3642903d1568d076c741dab8db/analysis/notes.Rmd" target="_blank">a2cde2d</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-02-17
</td>
<td>
end of day
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/a0936555e849a37dcbdf2ead14f6dd2e22b644b5/analysis/notes.Rmd" target="_blank">a093655</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-02-16
</td>
<td>
end of day
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/0935c13a4028690bb34cafa7c02c3b793c4d5bb8/analysis/notes.Rmd" target="_blank">0935c13</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-02-10
</td>
<td>
end of day
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/rgayler/fa_sim_cal/0d30c5bdc45b74957e36aec8f0b95ae7080f36c9/docs/notes.html" target="_blank">0d30c5b</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-26
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/rgayler/fa_sim_cal/23d740f2d8fd203c7453cb6cdd5428c9d9c4d1a4/docs/notes.html" target="_blank">23d740f</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-24
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/d0307f532f9997d04a892312867534577972227c/analysis/notes.Rmd" target="_blank">d0307f5</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-24
</td>
<td>
Add 02-1 char block vars
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/6df8db78ff0ddb04e22f2a305363710f6558cbb6/analysis/notes.Rmd" target="_blank">6df8db7</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-24
</td>
<td>
End of day
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/3090bea80592d42e0ce42f95196d698e3ac2fb10/analysis/notes.Rmd" target="_blank">3090bea</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-23
</td>
<td>
End of day
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/94b7b203b12a267994ce7cf52af63c70e08babe4/analysis/notes.Rmd" target="_blank">94b7b20</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-19
</td>
<td>
End of day
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/rgayler/fa_sim_cal/0c48ca570b817bf2cb9355916f449238c4456405/docs/notes.html" target="_blank">0c48ca5</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-17
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/305278040bdfedc7b3f210b100be2eade75b43ef/analysis/notes.Rmd" target="_blank">3052780</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-17
</td>
<td>
Add 02-1 block vars
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/rgayler/fa_sim_cal/3eb6e3b9d8f5ea5b8042c5aedf7b8254577c22f5/docs/notes.html" target="_blank">3eb6e3b</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-16
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/e6721d372917127574bdc931df4576a7bd2345c4/analysis/notes.Rmd" target="_blank">e6721d3</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-16
</td>
<td>
Add blocking to notes.Rmd
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/rgayler/fa_sim_cal/5ab5dc465e2faee2e8450ec148085b0ce634dee4/docs/notes.html" target="_blank">5ab5dc4</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-15
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/c674a51143f78ae013f84d528bfa8d4d55e42f87/analysis/notes.Rmd" target="_blank">c674a51</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-15
</td>
<td>
Add 01-6 clean vars
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/rgayler/fa_sim_cal/0b67848ad7f059c14f7f8ec0a7f4904b9a65200c/docs/notes.html" target="_blank">0b67848</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-06
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/ec38ffcf7c8c70f12ca927f7d762b01d2a52be00/analysis/notes.Rmd" target="_blank">ec38ffc</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-06
</td>
<td>
wflow_publish("analysis/no*.Rmd")
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/rgayler/fa_sim_cal/03ad32429914a222b6d099b1294971e974e5cdda/docs/notes.html" target="_blank">03ad324</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-05
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/rgayler/fa_sim_cal/blob/b03a2c1dd5445240cd6f89d8b8bf92d8a23ae83a/analysis/notes.Rmd" target="_blank">b03a2c1</a>
</td>
<td>
Ross Gayler
</td>
<td>
2021-01-05
</td>
<td>
wflow_publish(c(“analysis/index.Rmd”, “analysis/notes.Rmd”))
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<p>This document is for keeping notes of any points that may be useful for
later project or manuscript development and which are not covered in the
analysis notebooks or at risk of getting lost in the notebooks.</p>
<div id="project-infrastructure" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Project infrastructure</h1>
<ul>
<li>Consider using the <a href="https://wlandau.github.io/targets/">targets</a>
package to control the computational workflow.</li>
</ul>
</div>
<div id="entity-data" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Entity data</h1>
<ul>
<li><p>Get a sizeable publicly available data set with personal names
(NCVR).</p>
<ul>
<li>The focus of the empirical work is on string similarity metrics
of names.</li>
</ul></li>
<li><p>Use sex and age in addition to personal names so that most records
are discriminable.</p>
<ul>
<li><p>High frequency names will likely not be discriminable with only
these attributes.</p>
<ul>
<li>This is not a problem because we are really interested in
whether the methods proposed here assist in quantifying the
discriminability of records. We want records spanning a wide
range of discriminability.</li>
</ul></li>
<li><p>Age (and possibly sex) will be used as a blocking variable.</p>
<ul>
<li>Blocking is probably needed to make the project
computationally tractable.</li>
</ul></li>
<li><p>Age and sex are also of interest in the calculation of name
frequency because name distributions should vary conditional on
age and sex.</p></li>
</ul></li>
<li><p>Keep address and phone number as they may be useful for manually
checking identity in otherwise nondiscriminable records.</p>
<ul>
<li>As a fallback position, address and phone number can be used as
discriminating attributes in the compatibility model.</li>
</ul></li>
<li><p>Get the oldest available data to minimise it’s currency (NCVR 2005
snapshot).</p></li>
<li><p>Drop objectionable attributes such as race and political
affiliation.</p></li>
<li><p>A note about the irregularity of names:
<a href="https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/" class="uri">https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/</a></p></li>
</ul>
</div>
<div id="entity-data-preparation" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Entity data preparation</h1>
<ul>
<li><p>Apply basic data cleaning to the predictive attributes.</p>
<ul>
<li><p>This is probably unnecessary given how the data will be used.</p></li>
<li><p>I can’t bring myself to model data without scrutinising it
first.</p></li>
</ul></li>
<li><p>Only keep records that are ACTIVE and VERIFIED for modelling.</p>
<ul>
<li><p>These are likely to have the highest data quality attributes.</p></li>
<li><p>These are least likely to have duplicate records (i.e. referring
to the same person).</p></li>
</ul></li>
</ul>
</div>
<div id="blocking" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Blocking</h1>
<ul>
<li><p>Use blocking to reduce the number of comparisons, to keep the
computational cost feasible.</p></li>
<li><p>This project is not an operational system and we are only using
blocking to reduce the computational cost, so we can choose blocking
that would not be acceptable for an operational system.</p></li>
<li><p>Where the dictionary blocks vary widely in size we might choose to
work with only a subset of blocks that are a suitable size.</p></li>
<li><p>If we think that some aspect of the compatibility modelling might
vary as a function of block size we would probably want to test this
over a wide range of block sizes.</p></li>
<li><p>We will probably be repeating analyses over some number of blocks to
assess the variability of results, but that might only be a subset
of blocks with no commitment to examine <em>all</em> the blocks.</p></li>
<li><p>Blocking variables may have missing values in the query and
dictionary records.</p>
<ul>
<li>This can be handled but is fiddly and not the focus of this
project.</li>
<li>Handling missing <em>predictor</em> values in regression-based
compatibility models is simple.</li>
</ul></li>
<li><p>Exclude records with missing values on the blocking records from
use, so that missing values don’t have to be allowed for in
blocking.</p></li>
<li><p>Try to choose blocking variables with a small proportion of missing
values, so as to minimise systematic bias due to their exclusion.</p></li>
<li><p>Construct a few potentially useful blocking variables.</p></li>
<li><p>Blocking can induce changes in the distributions of names.</p>
<ul>
<li><p>Blocking on sex (in combination with other variables) will
definitely give more homogeneity of first names within blocks
because of gendered names.</p></li>
<li><p>Blocking on age will give more homogeneity of first names within
blocks because of name popularity varying over time.</p></li>
<li><p>Blocking on county <em>may</em> give more homogeneity of last names
within blocks because of families living together.</p></li>
</ul></li>
</ul>
</div>
<div id="entity-data-characterisation" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Entity data characterisation</h1>
<p>These sections are about looking at the properties of the entity data
that will be most relevant to their use in the compatibility models.</p>
<div id="structure-induced-by-equality" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Structure induced by equality</h2>
<ul>
<li><p>Prior work on name frequency has (implicitly) only considered
equality of names. That is two name tokens are either absolutely
identical or absolutely different. (No gradations of difference are
entertained.) Counting the frequency of names is finding the
cardinality of each set of identical name tokens.</p>
<ul>
<li>Name frequency will be used as a predictor in a compatibility
model, <span class="math inline">\(compat(q, d_i)\)</span>.</li>
<li>For a query record <span class="math inline">\(q\)</span> the compatibility will be estimated for
every dictionary record <span class="math inline">\(d_i\)</span> in the block <span class="math inline">\(B_q\)</span> (the set of
dictionary records in the block selected by the query).</li>
<li>The estimated compatibility will vary over <span class="math inline">\(d_i\)</span>, so we expect
the predictors to be functions of <span class="math inline">\(d_i\)</span> (in the context of the
set of dictionary records, <span class="math inline">\(B_q\)</span> selected by the query record.)</li>
<li>The (first attempt at) name equality frequency <span class="math inline">\(f_{eq}(q, d_i)\)</span>
is defined as
<span class="math inline">\(\vert \{ d_j : d_j \in B_q \land name(d_j) = name(d_i)\} \vert\)</span>.</li>
</ul></li>
<li><p>Look at frequency distributions of names conditional on name length.
The Zipf distributions may have different shape parameters for
different name lengths. Name length might be examined as an
alternative to name frequency for interaction with similarity.</p></li>
<li><p>Look at frequency distributions of names conditional on age and/or
sex.</p>
<ul>
<li>These conditional distributions may increase the predictive
power of the compatibility model.</li>
</ul></li>
<li><p>Look at frequency distributions of names conditional on blocking
variables.</p>
<ul>
<li>This is to get an understanding of the effect of blocking on
name frequency distributions.</li>
<li>In particular, look at any effects of block size (which has a
very wide range).</li>
<li>The anticipated usage is that name frequency will be calculated
within the dictionary block selected by the query record. (The
block can be construed as the only dictionary that matters for
the purposes of the query.)</li>
</ul></li>
</ul>
</div>
<div id="structure-induced-by-similarity" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Structure induced by similarity</h2>
<ul>
<li><p>The similarity version of name frequency is an extension of the
equality version. It counts the number of dictionary records in the
block that are at least as similar to the query record as the
currently considered dictionary record.</p>
<ul>
<li>The (first attempt at) name similarity frequency
<span class="math inline">\(f_{sim}(q, d_i)\)</span> is defined as
<span class="math inline">\(\vert \{ d_j : d_j \in B_q \land sim(name(q), name(d_j)) \ge sim(name(q), name(d_i))\} \vert\)</span>.</li>
</ul></li>
<li><p>Look at similarity frequency distributions of names. It’s not
obvious that these should be Zipf distributions. For example, the
rare names might be quite similar to more frequent names, which
might obscure the long tail of the underlying distribution of names.</p></li>
<li><p>Look at similarity frequency distributions of names conditional on
name length. The Zipf distributions may have different shape
parameters for different name lengths.</p>
<ul>
<li>This is of interest because similarity is usually scaled to be
between 1 (equality) and 0 (completely different) regardless of
the string length. The longer the strings the greater the size
of the space of possible strings and the higher the
dimensionality of the space. It’s not obvious to me that
equality (inequality) of very short strings carries the same
evidential value as equality (inequality) of long strings.</li>
</ul></li>
<li><p>Look at similarity frequency distributions of names conditional on
age and/or sex.</p>
<ul>
<li>These conditional distributions may increase the predictive
power of the compatibility model.</li>
</ul></li>
<li><p>Look at similarity frequency distributions of names conditional on
blocking variables.</p>
<ul>
<li>This is to get an understanding of the effect of blocking on
name frequency distributions.</li>
<li>In particular, look at any effects of block size (which has a
very wide range).</li>
</ul></li>
</ul>
</div>
</div>
<div id="sampling" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Sampling</h1>
<ul>
<li><p>Both modelling and performance evaluation will be based on random
samples of the entity data, so sort out the sampling first.</p></li>
<li><p>Modelling is necessarily based on query/dictionary record pairs.
That is, the record pair is the unit of analysis.</p></li>
<li><p>Performance assessment of model calibration will also be at the
record pair level.</p></li>
<li><p>Performance assessment of overall model performance at identifying
the correctly matching dictionary record (e.g. AUC or F-score) is at
the query record level.</p>
<ul>
<li>The query-level response is calculated by a computation across
all the dictionary/query record pairs induced by the query
record.</li>
<li>Assessment of query-level performance requires that the query
records be an appropriately defined random sample of the entity
data records.</li>
<li>The query records should be a uniform random sample of the
entity data records. Because to do otherwise is equivalent to
assuming that the queries are a systematically biased subset of
the universe of entities, which is another whole research topic
which we will not be addressing in this project.</li>
</ul></li>
<li><p>The query/dictionary record pairs are generated as combinations of a
set of queries with a fixed dictionary. The record pairs can’t be
uniformly sampled from the set of all possible record pairs because
that wouldn’t respect the dictionary definition.</p>
<ul>
<li>We need to create dictionaries by some sampling process; create
a set of queries by some sampling process; then those two sets
of records will jointly determine the query/dictionary record
pairs that are in-sample.</li>
</ul></li>
<li><p>We need some sort of training/testing partition to keep performance
assessment honest.</p></li>
<li><p>Remember that queries are run against a dictionary, so in principle,
it would be possible to have separate test/train queries run against
one fixed dictionary.</p>
<ul>
<li><p>I am inclined to think that running training/testing queries
against the same dictionary is probably OK because in a
practical application the modelling would be done with respect
to the dictionary as it existed at some point in time and then
queries applied against essentially the same dictionary. (Given
a large dictionary, subsequent queries would update it
relatively slowly.)</p></li>
<li><p>On the other hand, I am inclined to think that using separate
dictionaries for training/test would not cause a problem,
because the proposed models are not intended to rely on the
actual names in the dictionary, but on properties of the names
(like frequency and length), which are likely to be very similar
for different dictionaries based on random samples of the
universe of names.</p></li>
<li><p>On balance, I think I will have different dictionaries for
train/test because it is probably harmless, probably
conservative with respect to performance assessment, and
probably easy to implement.</p></li>
</ul></li>
<li><p>Dictionaries are constructed from entity data records, so having
separate train/test dictionaries implies partitioning the entity
data records prior to constructing the dictionaries.</p>
<ul>
<li><p>The easiest way to partition the records is to partition
independently at the record level.</p>
<ul>
<li>Note that records containing common names will occur in both
data sets (train and test).</li>
</ul></li>
<li><p>Partitioning the entity records so that each name type occurred
in only one of the train or test data sets would be the most
conservative in terms of separation of the train and test data
sets.</p>
<ul>
<li>This would be easy if there was only one name field.
However, we have three name fields (first, middle, last).
Partitioning the data so that all the names occurred in only
one data set would drastically reduce the number of records
able to be used and bias the data towards names that only
occur once (because it is impossible for those names to
occur in two data sets).</li>
</ul></li>
</ul></li>
<li><p>Partition the entity data records uniformly and independently into
training and testing data sets.</p>
<ul>
<li>Make the training and testing sets of equal size so that the
dictionary statistics are similar.</li>
<li>The construction of all subsamples of the training set will be
paralleled in the construction of subsamples of the testing set.</li>
</ul></li>
<li><p>We will want to repeat the entire analytic process multiple times on
different samples in order to assess the variability of the results.</p>
<ul>
<li><p>Repeat the partitioning into training and test sets for as many
replicates as are needed.</p></li>
<li><p>The number of replicates is yet to be determined.</p>
<ul>
<li><p>It needs to be large enough to give a reasonable view of
variability of the results.</p></li>
<li><p>It needs to be not so large as to be too computationally
expensive.</p></li>
</ul></li>
</ul></li>
<li><p>The dictionary will be a randomly sampled subset of the test or
train universe, because it is usual in reality for the dictionary to
be a subset of the universe of entities.</p></li>
<li><p>The dictionary sampling will be repeated at a range of sampling
rates. This will allow us to investigate calibration of the model as
the fraction of the universe present in the dictionary varies.</p>
<ul>
<li><p>It is expected that this can be captured as a change of value of
the intercept term of the logistic regression.</p></li>
<li><p>In practice, we usually don’t know the size of the universe
relative to the dictionary.</p></li>
<li><p>If this effect is captured as an intercept term it will be
simple to adjust the model to reflect the <em>assumed</em> size of the
universe.</p></li>
</ul></li>
<li><p>The queries are selected as a random subset of the relevant
universe.</p>
<ul>
<li><p>Use the same queries across all the corresponding dictionaries.</p>
<ul>
<li><p>This is marginally less effort than choosing different
queries per dictionary</p></li>
<li><p>This should result in the evaluations across dictionaries
being somewhat more similar than if the queries were chosen
separately for each dictionary, because the “difficulty” of
the queries is fixed across dictionaries.</p></li>
<li><p>The fraction of queries having matches in the dictionary
will (obviously) vary with the sampling rate that created
the dictionary.</p></li>
</ul></li>
<li><p>Sample some fixed number of queries (yet to be determined)</p>
<ul>
<li><p>The number of queries needs to be large enough to reasonably
cover the blocks in the directory, the range of the
predictors used in the model, and provide a reasonable
observations to parameters ratio for estimating the model.</p></li>
<li><p>The number of queries needs to be not so large as to be
infeasibly expensive to compute the models.</p></li>
</ul></li>
</ul></li>
</ul>
<p>The entire sampling process is summarised in the following diagram.</p>
<div id="htmlwidget-a38284e755db9bcdfd2d" style="width:672px;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-a38284e755db9bcdfd2d">{"x":{"diagram":"\ndigraph sampling_diagram {\n\nnode [shape = square, fixedsize = true, fontsize = 6]\n\ntotal  [label = \"total data set\", style = filled, fillcolor = gray]\n\ntt_1  [label = \"train/test\n50:50 splitter 1\", shape = triangle]\ntt_2  [label = \"train/test\n50:50 splitter 2\", shape = plaintext]\ntt_k  [label = \"train/test\n50:50 splitter k\", shape = plaintext]\n\ntotal -> {tt_1 tt_2 tt_k}\n\ntrain [label = \"train\nuniverse\", style = filled, fillcolor = gray]\ntest  [label = \"test\nuniverse\", style = filled, fillcolor = gray]\n\ntt_1 -> {train test}\n\ntrn_qry_samp [label = \"query\nsampler: n_q\", shape = triangle]\ntrn_ds_90    [label = \"dictionary\nsampler: 90%\", shape = triangle]\ntrn_ds_70    [label = \"dictionary\nsampler: 70%\", shape = plaintext]\ntrn_ds_50    [label = \"dictionary\nsampler: 50%\", shape = plaintext]\ntrn_ds_30    [label = \"dictionary\nsampler: 30%\", shape = plaintext]\ntrn_ds_10    [label = \"dictionary\nsampler: 10%\", shape = plaintext]\n\ntrain -> {trn_qry_samp trn_ds_90 trn_ds_70 trn_ds_50 trn_ds_30 trn_ds_10}\n\ntrn_qry   [label = \"train\nqueries\", style = filled, fillcolor = gray]\n\ntrn_qry_samp -> trn_qry\n\ntrn_dict [label = \"train\ndictionary:\n90%\", style = filled, fillcolor = gray]\n\ntrn_ds_90 -> trn_dict\n\ntst_qry_samp [label = \"query\nsampler: n_q\", shape = triangle]\ntst_ds_90    [label = \"dictionary\nsampler: 90%\", shape = triangle]\n\ntest -> {tst_qry_samp tst_ds_90}\n\ntst_qry  [label = \"test\nqueries\", style = filled, fillcolor = gray]\ntst_dict [label = \"test\ndictionary:\n90%\", style = filled, fillcolor = gray]\n\ntst_qry_samp -> tst_qry\ntst_ds_90    -> tst_dict\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p>The square nodes represent sets of entity data records.</p>
<p>The triangular nodes represent random sampling processes.</p>
<p>The text-only nodes represent replicates of random sampling nodes (just
made less visually obtrusive, for clarity).</p>
<p>The top data node represents the complete data set of entity data
records.</p>
<p>The “train/test 50:50 splitter 1” node randomly partitions the total
data into two data sets of (approximately) equal size: the “train
universe” and the “test universe”.</p>
<p>The “train/test 50:50 splitter” nodes represent the replication of the
train/test split as many times as are needed.</p>
<p>The “query sampler: n_q” node randomly samples <span class="math inline">\(n_q\)</span> records from the
“train universe” to be used as the “train queries” to be run against all
of the dictionaries sampled from the “train universe”.</p>
<p>The “dictionary sampler: 90%” node randomly selects 90% of the “train
universe” records to use as a dictionary.</p>
<p>The remaining “dictionary sampler” nodes create more dictionaries with
different selection rates.</p>
<p>The one query data set is applied to all the corresponding dictionary
data sets.</p>
<p>The construction of the query and dictionary data sets from the “test
universe” exactly parallels the construction from the “train universe”.</p>
<p>The construction of the data sets under each of the “train/test 50:50
splitter” nodes exactly parallels the construction under the “train/test
50:50 splitter 1” node.</p>
</div>
<div id="modelling" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Modelling</h1>
<ul>
<li><p>Try indicators for missingness. Missingness may be differentially
informative across different predictor variables.</p></li>
<li><p>Try indicators for similarity == 1. The compatibility of exact
string equality is not necessarily continuous with the compatibility
of similarity just below 1.</p></li>
<li><p>Try name frequency as an interactive predictor variable.</p>
<ul>
<li>Also consider frequency conditional on age and/or sex</li>
</ul></li>
<li><p>There are <em>two</em> names in each lookup: dictionary and query.
Therefore there are also two name frequencies to be considered.</p>
<ul>
<li><p>We are interested in calculating the probability of a correct
match for each query, so the sought probability is conditional
on the query. Consequently the conditional probability of a
correct match should <em>not</em> depend on the frequency of the query
name.</p></li>
<li><p>I am not entirely convinced by that argument, so consider how to
use <em>both</em> frequencies (e.g. min, max, geometric mean, …).</p></li>
<li><p>Queries may contain names that do not exist in the dictionary,
so we need to deal with that case.</p></li>
<li><p>Do we need to apply frequency smoothing, as used in
probabilistic linguistic models?</p></li>
<li><p>Do we need to estimate the probability mass of unobserved names?</p></li>
</ul></li>
<li><p>In general, the dictionary will be a subset of the entities in the
universe of queries. Consider the impact of this on modelling as the
fraction of the query universe in the dictionary varies.</p>
<ul>
<li><p>Can the sampling fraction be modelled as a prior probability
(effectively, a change in the intercept term of a logistic
regression)?</p>
<ul>
<li><p>This can be investigated by varying the fraction of
dictionary records randomly sampled from the universe and
treating the sampling fraction as a predictor in the model.
The fitted coefficients for the sampling fraction can be
compared to what would be expected treating the same
sampling probabilities as a prior for the logistic
regression.</p></li>
<li></li>
</ul></li>
<li><p>Consider whether this varies on a per query basis because of
blocking. That is, is there effectively a separate dictionary
per blocking value?</p></li>
<li><p>Consider using the same variable for blocking and as a predictor
to compare the effect on estimated probability of identity match
as a function of dictionary fraction.</p>
<ul>
<li>It is feasible to use the “same” variables for blocking and
as predictors. The blocks are based on the values of the
dictionary records and selected by the value in the query
record. The predictor variables are properties of record
<em>pairs</em> - so there will still be within-block variance of
the predictor even when there is no within-block variance of
properties of the query record. That is, they’re not the
“same” variable when they are used for blocking and as a
predictor.</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="performance-evaluation" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Performance evaluation</h1>
<ul>
<li><p>Performance evaluation should always be performed on a disjoint set
of records from those used in estimation of the model.</p>
<ul>
<li><p>The obvious (and probably best) way to partition the data is to
randomly select records. This means that for more frequent
names, the same name will appear in the fitting and testing
sets. So, if the model is in some sense “learning” details of
individual names, this knowledge will be carried over from the
fitting to testing sets and the performance of the model will be
overstated.</p></li>
<li><p>An alternative approach would be to partition the name types (as
opposed to tokens). This would ensure that the names in the
fitting and testing sets were completely disjoint, completely
excluding any possibility of name-specific information being
carried from fitting to testing.</p>
<ul>
<li>This would be easy if there were only one name field, but
there are three. Partitioning name types in the most obvious
way and making a 50:50 partition on each name field would
mean that only 1/8 of records would be in the fitting set
and 1/8 in the test set. This may make the data sets
uncomfortably small.</li>
</ul></li>
</ul></li>
<li><p>Partition the entity data records into a dictionary and a set of
queries (the <span class="math inline">\(Q_U\)</span> set).</p>
<ul>
<li>By the (reasonable) assumption that there are no duplicate
records, each of the <span class="math inline">\(Q_U\)</span> queries will be unmatched in the
dictionary.</li>
</ul></li>
<li><p>Select a subset of the dictionary records to use as the <span class="math inline">\(Q_M\)</span> query
set.</p>
<ul>
<li>By the (reasonable) assumption that there are no duplicate
records, each of the <span class="math inline">\(Q_M\)</span> queries will have exactly one
matching record in the dictionary.</li>
</ul></li>
<li><p>This is evaluation is different to the usual evaluation of entity
resolution in that it doesn’t consider the impact of
transcription/typographical variation in the queries.</p>
<ul>
<li>It looks at the quantification of discriminability when the
available attributes are not necessarily able to ensure that all
records are discriminable.</li>
</ul></li>
<li><p>If we are interested in the performance with respect to
transcription/typographical variation we may need to consider
artificially corrupting some of the queries.</p></li>
<li><p>Consider assigning some of the dictionary records to randomly chosen
wrong blocks. (Is this equivalent to randomly selecting some
out-of-block query records to run against each dictionary block?)</p></li>
</ul>
</div>
<div id="writingtheory" class="section level1" number="9">
<h1><span class="header-section-number">9</span> Writing/theory</h1>
<ul>
<li>Note relationship to Fellegi &amp; Sunter / probabilistic record linkage
(in proposal?).</li>
</ul>
<div id="quantity-to-be-modelled" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> Quantity to be modelled</h2>
<ul>
<li><p>The unit of analysis is a dictionary/query record pair.</p></li>
<li><p>The logistic regression model is modelling the probability that the
dictionary record refers to the same entity as the query record.</p>
<ul>
<li><p>That is, the record-pair modelling yields an unconditional
probability that does not depend on how that predicted
probability will be used (e.g. compared to a threshold).</p></li>
<li><p>However, the logic behind the name frequency induced by
similarity <em>does</em> appear to incorporate a dependence on usage.
The frequency is calculated as the sum of the frequncies of the
names (in the same block) that have similarity to the query name
greater than or equal to the query similarity to the dictionary
name being considered. This is based on an implicit <em>ceteris
paribus</em> usage of similarity: If I were to accept this
particular name as indicating an entity match, then logically I
should also accept all other names that are at least as similar.</p>
<ul>
<li><p>The name frequency induced as equality can be construed as
compatible with this definition by interpreting equality as
a degenerate similarity relation.</p></li>
<li><p>Is this a reasonable argument?</p></li>
<li><p>Should this implied comparison across names within block be
incorporated somehow in the mathematical formulae?</p>
<ul>
<li>This feels like it ought to be notated as a conditional
probability (i.e. conditional on the dictionary record
being accepted as a match) rather than an unconditional
probability.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="blocking-1" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> Blocking</h2>
<ul>
<li><p>It might be possible to develop (or at least explain) most of the
maths in terms of blocking.</p></li>
<li><p>Start with a universe <span class="math inline">\(E\)</span> of entities with the queries drawn
uniformly at random from that universe and the dictionary being
identical to the universe.</p></li>
<li><p>In the absence of any other information, the probability of each
dictionary element being an identity match with the query is $ 1 /
|E| $.</p></li>
<li><p>If the dictionary <span class="math inline">\(D\)</span> is a proper subset of <span class="math inline">\(E\)</span> it can be thought of
as a (not very informative) block. That is, a block can be thought
of as the dictionary induced by the query record.</p></li>
<li><p>If <span class="math inline">\(D\)</span> is a random subset of <span class="math inline">\(E\)</span>, and in the absence of any other
information, the probability of each dictionary element being an
identity match with the query is <span class="math inline">\((|D| / |E|) / |D| = 1 / |E|\)</span>.</p></li>
<li><p><span class="math inline">\(|D|\)</span> is the block size of the dictionary construed as a block. In
the absence of any information to discriminate between records in
the block, the larger the block, the lower the probability the any
block record is the identity match to the query record.</p></li>
<li><p><span class="math inline">\(|D| / |E|\)</span> is the probability that the entity corresponding to the
query record is in the dictionary/block, given that the dictionary
is a uniform random subset of the universe of entities.</p></li>
<li><p>Consider the other extreme, where blocking is perfect. That is, the
entity query record is guaranteed to be in the dictionary. In this
case the blocking is very informative (not a random selection from
the universe of entities). The probability that the correctly
matching entity record is in the block is 1, and the probability of
each dictionary/block element being an identity match with the query
is <span class="math inline">\(1 / |D|\)</span>.</p>
<ul>
<li><p>That is, with perfect blocking, the probability of each entity
being the identity match depends only on the block size.</p></li>
<li><p>Where blocking is based on, say, name, the block size is the
frequency of the name in the dictionary.</p></li>
<li><p>Where blocking is less then perfect the probability that the
entity corresponding to the query record is contained in the
block will be less than 1.</p></li>
</ul></li>
<li><p>This suggests that the probability of each record in the block being
the correct match could be calculated as the product of conditional
probabilities:</p>
<p><span class="math display">\[ P(id(q) = id(d_i)) = P(id(q) = id(d_i) | d_i \in B_j) P(d_i \in B_j | d_i \in D)  P(d_i \in D) \]</span></p></li>
<li><p>What we need are models to estimate those component probabilities. I
suspect that the cardinalities of those blocks would be very strong
predictors of the probabilities. If the blocks are defined in terms
of equality of names then the name frequencies determine the block
cardinalities.</p></li>
<li><p>If there were multiple independent blockings, the probability of
correct match of each record in the intersection of the blocks id
the product of the probabilities associated with each block. This is
equivalent to naive Bayes and shows the equivalence between
construing the problem as a multivariate regression and construing
it as using multiple blocking variables.</p>
<ul>
<li>Using regression rather than naive Bayes allows compensating for
the blocking variables not being independent.</li>
</ul></li>
</ul>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span>
Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre><code>R version 4.0.3 (2020-10-10)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 20.10

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0

locale:
 [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    
 [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   
 [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base     

other attached packages:
[1] DiagrammeR_1.0.6.1 here_1.0.1         workflowr_1.6.2   

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.6         rstudioapi_0.13    whisker_0.4        knitr_1.31        
 [5] magrittr_2.0.1     R6_2.5.0           rlang_0.4.10       stringr_1.4.0     
 [9] visNetwork_2.0.9   tools_4.0.3        xfun_0.21          git2r_0.28.0      
[13] htmltools_0.5.1.1  ellipsis_0.3.1     yaml_2.2.1         digest_0.6.27     
[17] rprojroot_2.0.2    tibble_3.0.6       lifecycle_1.0.0    bookdown_0.21     
[21] crayon_1.4.1       RColorBrewer_1.1-2 later_1.1.0.1      htmlwidgets_1.5.3 
[25] vctrs_0.3.6        promises_1.2.0.1   fs_1.5.0           glue_1.4.2        
[29] evaluate_0.14      rmarkdown_2.6      stringi_1.5.3      compiler_4.0.3    
[33] pillar_1.4.7       jsonlite_1.7.2     httpuv_1.5.5       renv_0.12.5       
[37] pkgconfig_2.0.3   </code></pre>
</div>
</div>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>




</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4,h5",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "site_libs/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
