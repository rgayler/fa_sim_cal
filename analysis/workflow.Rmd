---
title: "Workflow"
author: "Ross Gayler"
# date: "2021-02-26"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, echo = FALSE}
# Don't echo the chunk code in these notes
knitr::opts_chunk$set(echo = FALSE)

library(DiagrammeR)
```

This project uses the [`targets`](https://wlandau.github.io/targets/)
and [`workflowr`](https://github.com/jdblischak/workflowr) packages for
managing the workflow of the project (making sure that the dependencies
between computational steps are satisfied). When this work was started
there were no easily found examples of using `targets` and `workflowr`
together. This notebook contains notes on the proposed workflow for
using `targets` and `workflowr`.

# Assumptions

These points reflect my (possibly faulty) understanding of `targets` and
`workflowr`. If I am wrong here I hope that somebody will see this and
let me know, rather than me having to find out the hard way.

-   `targets` and `workflowr` both work by tracking some set of entities
    and the computational dependencies between them. When any of the
    tracked entities changes the packages calculate the minimal set of
    downstream dependencies that need to be recomputed to bring all the
    entities into a consistent state of being up to date.

-   `targets`

    -   Supports a computational-pipeline-centric style of analysis

    -   Tracks data objects (including files) and functions.

    -   The focus is on the data transformation by the computational
        pipeline (rather than human generated text in reports).

        -   Although reports can be rendered, they appear to be treated
            as optional final steps like generating a plot, rather than
            as a core concern.

    -   Knows about high-performance computing and can run computations
        in parallel.

-   `workflowr`

    -   Supports a notebook-centric style of analysis

    -   Only tracks Rmd notebook files and the corresponding rendered
        output files
        (<https://github.com/ropensci/tarchetypes/issues/23#issuecomment-749118599>)

        -   `workflowr::wflow_build()` tracks modification dates of Rmd
            files and the corresponding rendered output files
        -   `workflowr::wflow_publish()` tracks git status of Rmd files
            and the corresponding rendered output files
        -   The computational consistency aspect are really only about
            the consistency between the notebook Rmd files and their
            rendered counterparts

    -   The computational reproducibility aspect is restricted to
        ensuring that random number seeds are set appropriately, that
        each notebook is executed in a clean environment, and that the
        package versions are recorded

    -   Automatic building of a website for the rendered notebooks

    -   Publication of website integrated with git

    -   Automatic publication of website served by GitHub Pages

-   Comparison

    -   `targets` provides more general and fine-grained control of
        computational pipeline

        -   Rendering of Rmd documents can be treated as just another
            step in a `targets` computational pipeline
        -   I would prefer to have computational dependency tracking
            handled by one package rather than splitting responsibility
            across multiple packages
        -   If I am going to use only one package to handle
            computational dependency tracking then I think it has to be
            `targets`

    -   If I use `targets` to manage computational dependency tracking,
        what extra capabilities does `workflowr` provide?

        -   Automatic generation of a website mof rendered notebooks
        -   Automatic publication of the website via GitHub pages
            (provided you use GitHub as the git remote repository).
        -   I want this project to be publicly accessible and I don't
            want the trouble of having to manually generate a website,
            so I will use `workflowr` to handle the building and
            publication of a project website.

# Proposed workflow

-   The proposed workflow needs to support my preferences for how to
    organise a project. In particular, a computational research project
    necessarily involves many design choices for the computational
    details. It is my strong preference that the *reasoning* behind
    these design choices (which *may* involve additional empirical work
    to support the reasoning) is documented as part of the project.

-   The total workflow of the project has multiple components:

    -   **core** - This is the workflow that generates the primary
        computational outputs (data sets, tables, plots, etc.) of the
        project. None of the outputs of this workflow include manual
        interpretation. The transformations are purely mechanical and
        can be repeated automatically.
    -   **publications** - These are documents (manuscripts,
        presentations, etc.) that interpret the results of the core for
        some audience. These should be construed as the principal
        outputs of the core but they are treated separately because they
        necessarily involve interpretation which cannot be automated.
        The computation which generates the publications can be
        automated, but it can't automatically update the interpretations
        in the publications.
    -   **meta** - These are computations and interpretations that are
        *about* the core, but not required by the core.

====

Having thought some more about it overnight, I think mentioning
"history" in my earlier comments is misleading. I don't need a copy of
everything I have ever done (it's not like a financial transaction audit
trail). Rather, I need a record of the reasoning behind the current
design of the computational core pipeline, where:

-   The design of the computational core pipeline and the reasoning
    supporting that design are potentially revised over the lifetime of
    the project.

-   The current design and it's supporting reasoning are assumed to be
    my best current beliefs and improvements over any previous design
    and supporting reasoning.

-   Consequently, I don't need easy and immediate access to the prior
    designs and the prior supporting reasoning. git archaeology is fine
    if I ever need access to those earlier versions of the pipeline and
    design reasoning.

-   Some aspects of the current design may have been decided much
    earlier in the lifetime of the project.

-   Decisions about the design and supporting reasoning for the design
    are based (at least partly) on analyses of data available in the
    computational core pipeline.

-   I want the ability to run any of those design support analyses
    currently even if they relate to decisions that were taken long ago
    in the lifetime of the project.

I think I can support all that with {targets} primarily being used to
manage the computational core pipeline, and {workflowr} Rmd notebooks
hanging off the pipeline as side-branches.

The entire sampling process is summarised in the following diagram.

```{r sampling_diagram}
DiagrammeR::grViz("
digraph sampling_diagram {

node [shape = square, fixedsize = true, fontsize = 6]

total  [label = 'total data set', style = filled, fillcolor = gray]

tt_1  [label = 'train/test\n50:50 splitter 1', shape = triangle]
tt_2  [label = 'train/test\n50:50 splitter 2', shape = plaintext]
tt_k  [label = 'train/test\n50:50 splitter k', shape = plaintext]

total -> {tt_1 tt_2 tt_k}

train [label = 'train\nuniverse', style = filled, fillcolor = gray]
test  [label = 'test\nuniverse', style = filled, fillcolor = gray]

tt_1 -> {train test}

trn_qry_samp [label = 'query\nsampler: n_q', shape = triangle]
trn_ds_90    [label = 'dictionary\nsampler: 90%', shape = triangle]
trn_ds_70    [label = 'dictionary\nsampler: 70%', shape = plaintext]
trn_ds_50    [label = 'dictionary\nsampler: 50%', shape = plaintext]
trn_ds_30    [label = 'dictionary\nsampler: 30%', shape = plaintext]
trn_ds_10    [label = 'dictionary\nsampler: 10%', shape = plaintext]

train -> {trn_qry_samp trn_ds_90 trn_ds_70 trn_ds_50 trn_ds_30 trn_ds_10}

trn_qry   [label = 'train\nqueries', style = filled, fillcolor = gray]

trn_qry_samp -> trn_qry

trn_dict [label = 'train\ndictionary:\n90%', style = filled, fillcolor = gray]

trn_ds_90 -> trn_dict

tst_qry_samp [label = 'query\nsampler: n_q', shape = triangle]
tst_ds_90    [label = 'dictionary\nsampler: 90%', shape = triangle]

test -> {tst_qry_samp tst_ds_90}

tst_qry  [label = 'test\nqueries', style = filled, fillcolor = gray]
tst_dict [label = 'test\ndictionary:\n90%', style = filled, fillcolor = gray]

tst_qry_samp -> tst_qry
tst_ds_90    -> tst_dict
}
")
```

# Open questions

These are not immediately relevant to how I think the current project
will pan out, but I could easily imagine them being relevant in other
projects.

These questions arise because of my earlier assumption that `targets`
and `workflowr` are focused on creating a current status of the project
that is computationally consistent (up to date). Consequently, I have
assumed that the current status of the project is directly accessible,
but prior consistent states of the project are not directly (and
therefore not easily) accessible from within the project. The questions
below relate to use-cases where we would want current and prior states
of the project to be simultaneously and directly available.

These questions are based on the assumption that it's advantageous to
have the *entire* computational process managed by `targets` or
`workflowr` to ensure that everything is in a consistent state. That is,
I am trying to avoid having any computational processes that are not
managed by `targets` or `workflowr`.

I suspect that [`tarchetypes`](https://github.com/ropensci/tarchetypes)
and branching may be relevant to these questions.

## Accumulating historical/indexed states

Imagine a computational pipeline that ingests some data and reports on
it. The current output reports reflect the current input data.

Now imagine that the input data is regularly updated. Whenever the input
data is updated the output reports would also be updated so that the
previous output no longer exists in the pipeline environment. However,
in that use-case it is generally required that all the generated reports
continue to be available.

The results could be accumulated outside the computational pipeline but
that would appear to mean that part of the computational process is not
visible to and managed by `targets` or `workflowr`.

-   So, is there a reasonable way in `targets` or `workflowr` to
    accumulate an arbitrary number of analyses/results from the same
    pipeline?

-   As a related question, would that support regenerating *all* the
    reports if the pipeline functions were updated (e.g. a bug was
    fixed)?

-   Related to the previous question: Would it be possible to accumulate
    reports corresponding to different definitions of the pipeline
    functions (e.g. applying different modelling techniques to the
    data)?

The last question makes clear that referring to "historical" is somewhat
misleading. It would more generally be thought of as (potentially
multidimensional) indexing across data sets and pipeline definitions.

This might be easier to do if it was conceptualised as a point in time
computation with the indexing variable(s) used as a grouping variable(s)
in one input data set. However, that point in time view would require
recomputing all the outputs when the input is updated, even though
recomputing the previous outputs is unnecessary.

## Comparing historical/indexed states

The previous questions dealt with the case where the outputs are just
accumulated. Now consider the case where outputs for different index
values are combined computationally inside the pipeline managed by
`targets` or `workflowr`. This might be used to look at how output
values change over time or over changes in the pipeline definition.

-   Is there a reasonable way in `targets` or `workflowr` to accumulate
    an arbitrary number of analyses/results, indexed by data sets and/or
    pipeline definitions, from the same pipeline such that indexed
    results can be computational combined in later steps of the same
    pipeline?

# Resources

-   Consider using the [targets](https://wlandau.github.io/targets/)
    package to control the computational workflow.

    -   GitHub issue about using {targets} and {workflowr} together:
        <https://github.com/jdblischak/workflowr/issues/238>
    -   GitHub repo using {targets} and {rticles} together:
        <https://github.com/b-rodrigues/covid_pred>
