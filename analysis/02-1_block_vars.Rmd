---
title: "02-1_block_vars"
subtitle: "Characterise blocking variables"
author: "Ross Gayler"
date: "2021-01-15"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup}
# Set up the project environment, because each Rmd file knits in a new R session
# so doesn't get the project setup from .Rprofile

# Project setup
library(here)
source(here::here("code", "setup_project.R"))

# Extra set up for the 02*.Rmd notebooks
# source(here::here("code", "setup_02.R"))

# Extra set up for this notebook
# ???

# start the execution time clock
tictoc::tic("Computation time (excl. render)")
```

# Introduction

This notebook (`02-1_block_vars`) characterises the potential blocking
variables.

Conceptually, the compatibility of the query record is calculated with
respect to *every* dictionary record. Each compatibility calculation
might be computationally expensive, meaning that calculation could be
infeasibly expensive for large dictionaries. This is especially the case
if the identity query is an online, real-time transaction that requires
a very rapid response.

For a large dictionary of person records it is generally the case that
the vast majority of records have a very low compatibility with the
query record. That is, the vast majority of dictionary records have a
very low probability of being a correct match to the query record. If we
had a computationally cheap method for identifying these records we
could ignore them and only calculate the compatibility for the small set
of records with a reasonable probability of being a correct match to the
query record.

## Blocking

A common approach to this issue is to use blocking variables. A blocking
variable is a variable that exists in the dictionary and query records,
which takes a large number of values, such that the number of dictionary
records in each block is relatively small, and the correctly matching
dictionary record has the same blocking value as the query record (with
very high probability).

Any entity *should* have the same blocking value in the corresponding
dictionary and query records. Obviously this is not necessarily the case
if the record observation process is noisy for that variable.
Consequently, we should choose blocking variables that we expect to be
reliably observed in the dictionary and query records. There are
techniques for compensating for noisy blocking variables but they will
not be considered in this project.

There is room for pragmatic considerations in the choice of blocking
variables. For example, we might expect there to be some errors in date
of birth. However, in some high-stakes settings (e.g transferring
ownership of a very valuable asset) we might insist that date of birth
match exactly even though we realise that this will cause an increased
level of incorrect non-matches. Given the commitment to exact match of
data of birth we could then use date of birth as a blocking variable.

Blocking variables can be composite variables, that is, the product of
several variables. A person's gender would not normally be useful as a
blocking variable because it would only create a small number of large
blocks. However, it could usefully be combined with date of birth to
yield roughly twice as many blocks of half the size.

In this project we are not addressing issues of speed of response, so
could ignore blocking. However, we will use blocking for the practical
reason of reducing the computational cost of the calculations. This may
also expose issues which will turn up when blocking is used in real
applications.

We could reduce the computational cost by taking random samples of
records (effectively using random numbers to define blocks). However
this would lead to relatively uniform block sizes and the contents of
all blocks being relatively similar. When real properties of the
entities are used as blocking variables it means that the records in the
same block are necessarily more similar to each other in that attribute.
This may also induce greater variation in the block sizes.

The demographic variables `sex`, `age`, `birth_place`, and the
administrative variable `county_id` may be used as blocking variables in
this project. This is not a claim that they are good blocking variable
or that they *should* be used as blocking variables in any practical
application. We are using blocking variables in this project only for
the practical effect of reducing computational effort.

This notebook characterises those variables with respect to their
properties that are relevant to their use as potential blocking
variables. The selection and use of the blocking variables will occur
much later in the project.

## Characterisation

The distribution of block sizes is important in a practical setting
because it directly controls the computational cost. Large blocks are
computationally expensive and prevent real-time response, so should be
avoided. However, for the current project real-time response is not
relevant.

As this is a research project, we are not forced to use *every* block
that is induced by a blocking variable. We can select the blocks we use
to meet our requirements. The following issues are relevant to the
selection of blocks:

-   We want the largest blocks we select to be as large as possible
    (because in statistical modelling, more observations is generally
    better), subject to the constraint of not being so large as to be
    computationally impractical.

-   We want as wide a range of block sizes as possible because
    properties of the proposed entity resolution algorithm may vary as a
    function of block size.

-   We don't want the smallest blocks to be too small because we will be
    looking at the frequencies of values (e.g. names) calculated
    *within* the block. Those calculations probably become less
    meaningful as the blocks get smaller.

-   We want there to be a reasonable number of blocks of approximately
    every size that we use. This allows the analyses to be replicated
    over blocks to allow estimation of the variability of the results.

Unfortunately, we don't currently know what the upper and lower bounds
on practical block size should be. I will pull some numbers from the air
and say the usable block sizes should be between 50 and 50,000 records.

At this stage we should be cataloguing the distributions of block sizes
to allow us to make informed choices later.

# Read data

Read the usable data. Remember that this consists of only the ACTIVE &
VERIFIED records.

```{r}
# Show the entity data file location
# This is set in code/file_paths.R
fs::path_file(f_entity_cln_fst)

# get entity data
d <- fst::read_fst(
  f_entity_cln_fst,
  columns = c("sex", "age_cln", "age_cln_miss", "birth_place", "county_id")
  ) %>% 
  tibble::as_tibble()

dim(d)
```

# Univariate blocking

For each potential blocking variable, look at the distribution of
values, and comment on any properties that variable may have when used
for blocking.

## Sex

```{r}
# count the records in each level
x <- d %>% 
  dplyr::count(sex, sort = TRUE) %>% 
  dplyr::mutate(p = n / sum(n)) # convert count to probability

# number of levels
nrow(x)

# show the counts at each level
x %>% 
  knitr::kable(digits = 3, format.args = list(big.mark = ","))
```

-   Sex would not be used as a blocking variable in isolation because it
    has only 3 levels.

    -   Almost all the probability mass is in 2 levels.

-   Sex could be used in combination with other blocking variables. The
    effect would to roughly double the number of blocks and halve the
    block size.

-   If used in practice the small UNK group would have to be dealt with.
    For example, if it is genuinely "unknown" then the UNK records
    should probably be added to both the FEMALE and MALE blocks, which
    would mean that sex is no longer a partition of the records.

-   Blocking on sex is likely to induce greater within-block homogeneity
    of first and (to a lesser extent) middle names, because some first
    names are relatively specific to a gender.

## Age

```{r}
# arbitrary lower and upper bound on useful block size
blk_sz_min <- 50
blk_sz_max <- 50e3

# distribution of age
d %>% 
  ggplot() +
  geom_hline(yintercept = c(blk_sz_min, blk_sz_max), colour = "blue") +
  geom_histogram(aes(x = age_cln), binwidth = 1)

# distribution of age
d %>% 
  ggplot() +
  geom_hline(yintercept = c(blk_sz_min, blk_sz_max), colour = "blue") +
  geom_histogram(aes(x = age_cln), binwidth = 1) +
  scale_y_log10() + annotation_logticks(sides = "l")

# count the records in each level/block
d_blk <- d %>% dplyr::count(age_cln, name = "blk_sz", sort = TRUE)

# number of blocks
nrow(d_blk)

# number of blocks in "useful" size range
d_blk$blk_sz %>% dplyr::between(blk_sz_min, blk_sz_max) %>% sum()

# distribution of block sizes
summary(d_blk$blk_sz)

d_blk %>% 
  ggplot(aes(x = blk_sz)) +
  geom_vline(xintercept = c(blk_sz_min, blk_sz_max), colour = "blue") +
  geom_histogram(aes(y = ..density..), bins = 10) +
  geom_density(colour = "orange") +
  scale_x_log10()
```

-   There are 89 unique levels (blocks).

-   There are 43 blocks with sizes in the "useful" range.

    -   The blocks corresponding to ages 18 to 63 inclusive are larger
        than the "useful" range.

    -   In a real application we would likely have date of birth rather
        than age. This would give many more, much smaller blocks.

-   Age could be used in combination with other blocking variables, e,g,
    sex.

-   If used in practice the "missing" group (age = 0) would have to be
    dealt with. For example, if it is genuinely "unknown" then the 0
    records should probably be added to each of the other blocks.

-   Blocking on age is likely to induce greater within-block homogeneity
    of first and (to a lesser extent) middle names, because the
    fashionability of first names varies over time.

-   It is possible that blocking on age *might* induce greater
    within-block homogeneity of last names if, for example, there is
    time-varying immigration of groups with distinctive family names.

## Birth place

```{r}
# distribution of birth place
d %>% 
  dplyr::mutate(birth_place = forcats::fct_infreq(birth_place)) %>% 
  ggplot() +
  geom_hline(yintercept = c(blk_sz_min, blk_sz_max), colour = "blue") +
  geom_bar(aes(x = birth_place)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

# count the records in each level/block
d_blk <- d %>% dplyr::count(birth_place, name = "blk_sz", sort = TRUE)

# number of blocks
nrow(d_blk)

# number of blocks in "useful" size range
d_blk$blk_sz %>% dplyr::between(blk_sz_min, blk_sz_max) %>% sum()

# distribution of block sizes
summary(d_blk$blk_sz)

d_blk %>% 
  ggplot(aes(x = blk_sz)) +
  geom_vline(xintercept = c(blk_sz_min, blk_sz_max), colour = "blue") +
  geom_histogram(aes(y = ..density..), bins = 10) +
  geom_density(colour = "orange") +
  scale_x_log10()
```

-   There are 57 unique levels.

-   There are 44 blocks with sizes in the "useful" range.

    -   **But** \~46% of the probability mass is in one block (born in
        North Carolina).
    -   This would not be used in practice as a blocking variable.

-   We *could* exclude the North Carolina block from experiments, but
    that makes me uneasy as a source of systematic bias. I would prefer
    to use data that is closer to being representative of the NCVR data.

-   If used in practice the "missing" group would have to be dealt with.
    For example, if it is genuinely "unknown" then those records should
    probably be added to each of the other blocks.

## County ID

```{r}
# distribution of county ID
d %>% 
  dplyr::mutate(county_id = forcats::fct_infreq(county_id)) %>% 
  ggplot() +
  geom_hline(yintercept = c(blk_sz_min, blk_sz_max), colour = "blue") +
  geom_bar(aes(x = county_id)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

d %>% 
  dplyr::mutate(county_id = forcats::fct_infreq(county_id)) %>% 
  ggplot() +
  geom_hline(yintercept = c(blk_sz_min, blk_sz_max), colour = "blue") +
  geom_bar(aes(x = county_id)) +
  scale_y_log10() + annotation_logticks(sides = "l") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

# count the records in each level/block
d_blk <- d %>% dplyr::count(county_id, name = "blk_sz", sort = TRUE)

# number of blocks
nrow(d_blk)

# number of blocks in "useful" size range
d_blk$blk_sz %>% dplyr::between(blk_sz_min, blk_sz_max) %>% sum()

# distribution of block sizes
summary(d_blk$blk_sz)

d_blk %>% 
  ggplot(aes(x = blk_sz)) +
  geom_vline(xintercept = c(blk_sz_min, blk_sz_max), colour = "blue") +
  geom_histogram(aes(y = ..density..), bins = 10) +
  geom_density(colour = "orange") +
  scale_x_log10()
```

-   There are 100 unique levels (blocks).

-   There are 75 blocks with sizes in the "useful" range.

    -   The distribution of block sizes is not as wide as the "useful"
        range and is shifted to the high end of the range.
    -   More blocks would be a useful size if the county ID was
        coombined with another blocking variable, e.g. sex.

-   Blocking on county ID *might* induce greater within-block
    homogeneity of first and last names if, for example, family members
    tend to live near each other and certain names are popular within
    that family.

# Multivariate blocking

Look at the distributions of block sizes for all reasonable combinations
of potential blocking variables.

-   Don't consider place of birth because the distribution of levels is
    too concentrated.
-   Only consider sex in combination with other variables, not in
    isolation.

```{r}
# Calculate all the block sizes for each combination of blocking variables.
# For only 6 combinations I will manually enumerate them.
d_blk <- dplyr::bind_rows(
  list(
    sex_age_cnty = dplyr::count( d, sex, age_cln, county_id, name = "blk_sz"),
    sex_age      = dplyr::count( d, sex, age_cln           , name = "blk_sz"),
    sex_cnty     = dplyr::count( d, sex,          county_id, name = "blk_sz"),
    age_cnty     = dplyr::count( d,      age_cln, county_id, name = "blk_sz"),
    age          = dplyr::count( d,      age_cln           , name = "blk_sz"),
    cnty         = dplyr::count( d,               county_id, name = "blk_sz")
  ),
  .id = "blk_vars"
) %>% 
  dplyr::mutate(
    blk_vars = forcats::fct_infreq(blk_vars)
  )

d_blk %>% 
  ggplot(aes(x = blk_vars, y = blk_sz)) +
  geom_hline(yintercept = c(blk_sz_min, blk_sz_max), colour = "blue") +
  geom_jitter(width = 0.4, alpha = 0.1) +
  geom_boxplot(outlier.shape = NA, colour = "red", alpha = 0.0) +
  scale_y_log10(n.breaks = 7) + annotation_logticks(sides = "l") +
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.y = element_line())

d_blk %>% 
  ggplot(aes(x = blk_sz)) +
  geom_vline(xintercept = c(blk_sz_min, blk_sz_max), colour = "blue") +
  geom_histogram(aes(y = ..density..), bins = 10) +
  geom_density(colour = "orange") +
  scale_x_log10() +
  facet_grid(rows = vars(blk_vars))

# Characterise the blocking schemes
d_blk_summ <- d_blk %>% 
  dplyr::group_by(blk_vars) %>% 
  dplyr::summarise(
    n_blk = n(),
    n_rec = sum(blk_sz),
    
    min_blk_sz = min(blk_sz),
    max_blk_sz = max(blk_sz),
    
    n_blk_low = sum(blk_sz < blk_sz_min),
    p_blk_low = (n_blk_low / n_blk) %>% round(2),
    n_rec_low = sum(blk_sz[blk_sz < blk_sz_min]),
    p_rec_low = (n_rec_low / n_rec) %>% round(2),

    n_blk_high = sum(blk_sz > blk_sz_max),
    p_blk_high = (n_blk_high / n_blk) %>% round(2),
    n_rec_high = sum(blk_sz[blk_sz > blk_sz_max]),
    p_rec_high = (n_rec_high / n_rec) %>% round(2),
    
    n_blk_useful = sum(between(blk_sz, blk_sz_min, blk_sz_max)),
    p_blk_useful = (n_blk_useful / n_blk) %>% round(2),
    n_rec_useful = sum(blk_sz[between(blk_sz, blk_sz_min, blk_sz_max)]),
    p_rec_useful = (n_rec_useful / n_rec) %>% round(2),
    
    gm_useful_blk_sz = blk_sz[between(blk_sz, blk_sz_min, blk_sz_max)] %>% 
      log() %>% mean() %>% exp() %>% round() # geometric mean of useful block sizes
  )

# number of blocks
d_blk_summ %>% 
  dplyr::select(blk_vars, n_blk, n_blk_low, n_blk_high, n_blk_useful) %>% 
  knitr::kable(format.args = list(big.mark = ","))

# proportion of blocks
d_blk_summ %>% 
  dplyr::select(blk_vars, n_blk, p_blk_low, p_blk_high, p_blk_useful) %>% 
  knitr::kable(format.args = list(big.mark = ","))

# number of records
d_blk_summ %>% 
  dplyr::select(blk_vars, n_rec_low, n_rec_high, n_rec_useful) %>% 
  knitr::kable(format.args = list(big.mark = ","))

# proportion of records
d_blk_summ %>% 
  dplyr::select(blk_vars, p_rec_low, p_rec_high, p_rec_useful) %>% 
  knitr::kable(format.args = list(big.mark = ","))

# block size
d_blk_summ %>% 
  dplyr::select(blk_vars, min_blk_sz, max_blk_sz, gm_useful_blk_sz) %>% 
  knitr::kable(format.args = list(big.mark = ","))
```

-   county and age have too many blocks/records in the high block size
    range.

-   (sex, age, county) and (age, county) have too many blocks/records in
    the low block size range.

    -   This can be fixed by pooling the smallest levels of the
        individual variables.
    -   Small blocks can be made larger by pooling.
    -   Large blocks can not be made smaller without combining another
        blocking variable.

-   The largest block sizes for (sex, age, county) and (age, county) are
    quite small relative to the upper limit of the useful block size
    range.

-   (sex, age) and (sex, county) have the highest proportion of usefully
    sized blocks.

-   (sex, age) has \~100% of records in useful sized blocks.

-   (sex, county) has only 64% of records in useful sized blocks.

**Conclusion**

My tentative conclusion is to use (sex, age) for blocking and to pool
the higher ages into groups to ensure that all block sizes are above the
lower limit of the useful block size.

# (sex, age)

Try pooling some of the highest ages to increase the smallest block sizes above the lower limit of useful block size.

We don't need to use the same age pooling for males and females

# Timing {.unnumbered}

```{r echo=FALSE}
tictoc::toc()
```
